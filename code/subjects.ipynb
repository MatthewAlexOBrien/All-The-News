{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "subjects.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOWLUq0tKRJXTI1XsHh1eb9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatthewAlexOBrien/All-The-News/blob/master/code/subjects.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJPbJMimYeuE"
      },
      "source": [
        "# **1A. Identifying Named Subjects**\n",
        "\n",
        "> We start by extracting the subject of each article sentence using Stanford NLP Group's dependency parser pipeline, availble through their python NLP package 'Stanza'. https://stanfordnlp.github.io/stanza/depparse.html. Next, we use Stanza's Named Entity Regognition pipline to determine which nominal subjects are in fact named entities https://stanfordnlp.github.io/stanza/ner.html. Sentences for which the nominal sibject is not named-entity are removed from the dataset.\n",
        "\n",
        "\n",
        "*   Input: 50k subset of News Article Dataset. Has columns [date, year, month, day, title, article, url, section, publication]\n",
        "*   Output: Sentence dataset with sentence subjects. Has columns [year, month, day, publication, article_id, sentence_id, sentence, article_names_partial, article_names_full, sentence_subject_names]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHNU8d8l2Ajv"
      },
      "source": [
        "**Install and Import Libraries**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lil2uXTZ2LMK"
      },
      "source": [
        "%%capture\n",
        "\n",
        "# Packages not pre-install on Python 3.7\n",
        "!pip3 install stanza\n",
        "!pip3 install nltk\n",
        "!pip3 install ethnicolr\n",
        "!pip3 install vaderSentiment\n",
        "!pip3 install afinn\n",
        "!pip3 install gender-guesser\n",
        "!pip3 install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmsjC47F_iBj"
      },
      "source": [
        "%%capture\n",
        "\n",
        "# Imports\n",
        "import itertools\n",
        "import csv\n",
        "import sys\n",
        "import re\n",
        "import stanza\n",
        "import pandas\n",
        "from stanza.models.common.doc import Document\n",
        "import nltk\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "import glob\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "stanza.download(\"en\")\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KrNT5roBzmb"
      },
      "source": [
        "**Static Functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgNVyb6NB_AG"
      },
      "source": [
        "# Method to replace a partial names in a list with corresponding full names found in itself\n",
        "def partial_to_full(names, fullnames=None):\n",
        "    name_list = []\n",
        "    partial_name_to_full_name = dict()\n",
        "    usenames = names if fullnames==None else fullnames\n",
        "    for name in usenames:\n",
        "        parts_of_name = name.split()\n",
        "        name_length = len(parts_of_name)\n",
        "        all_combinations = []\n",
        "        for L in range(1, name_length+1):\n",
        "            for subset in itertools.combinations(parts_of_name, L):\n",
        "                subset = ' '.join(subset)\n",
        "                all_combinations.append(subset)  \n",
        "        for part in all_combinations:\n",
        "            if part not in partial_name_to_full_name:\n",
        "                partial_name_to_full_name[part] = name\n",
        "            elif part in partial_name_to_full_name:\n",
        "                if len(name) > len(partial_name_to_full_name[part]):\n",
        "                    partial_name_to_full_name[part] = name\n",
        "                else:\n",
        "                    pass\n",
        "            else:\n",
        "                pass   \n",
        "    for name in names:\n",
        "        name_list.append(partial_name_to_full_name.get(name))\n",
        "    return name_list\n",
        "\n",
        "\n",
        "# Method to explode a dataframe text column into sentences\n",
        "def split_sentences(data, textcol):\n",
        "    nlp_splitter = spacy.load('en_core_web_sm')\n",
        "    rows_list = []\n",
        "    def splitter(data = data):\n",
        "        doc = nlp_splitter(data[textcol])\n",
        "        a = [str(sent) for sent in doc.sents]\n",
        "        b = len(a)\n",
        "        dictionary = {\"article_id\": np.repeat(data.article_id,b), \"sentence_id\": list(range(1, b+1)), \"sentence\": a}\n",
        "        dictionaries = [{key : value[i] for key, value in dictionary.items()} for i in range(b)]\n",
        "        for dictionary in dictionaries:\n",
        "            rows_list.append(dictionary)\n",
        "    data.apply(lambda x: splitter(x), axis = 1)\n",
        "    sentences = pandas.DataFrame(rows_list, columns=['article_id', 'sentence_id','sentence'])\n",
        "    sentences = sentences.merge(data, on='article_id', how='left')\n",
        "    sentences = sentences.drop(textcol, 1)\n",
        "    return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmsHN6pIEY0j"
      },
      "source": [
        "**Article Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHusyPo0i1zy"
      },
      "source": [
        "class Article():\n",
        "    def __init__(self, data):\n",
        "        self.raw = data\n",
        "        \n",
        "    def names(self):\n",
        "        docs = nlp_stanza([Document([], text=doccontent) for doccontent in self])\n",
        "        article_names = [[f'{ent.text}' for ent in doc.ents if f'{ent.type}'==\"PERSON\"] for doc in docs]\n",
        "        return article_names \n",
        "    \n",
        "    def subject_names(self, fullnames):\n",
        "        article_names_subjects = []\n",
        "        relations = ['nsubj']\n",
        "        for article, fulls in zip(self, fullnames):\n",
        "            for name in fulls:\n",
        "                if len(name.split()) > 1:\n",
        "                    try:\n",
        "                      name = re.sub('[\\(\\[\\)\\]]', '', name)\n",
        "                      name_no_space = re.sub(\" \", \"\", name)\n",
        "                      article = re.sub(str(name), str(name_no_space), article)\n",
        "                    except:\n",
        "                      pass\n",
        "                else:\n",
        "                    pass\n",
        "            doc = nlp_spacy(article)\n",
        "            try:\n",
        "              subjects = [word.text for word in doc if word.dep_ in relations]\n",
        "              subjects = [re.sub(r\"(?<![A-Z])(?<!^)([A-Z])\",r\" \\1\",subject) for subject in subjects]\n",
        "              subjects = [subject for subject in subjects if subject in fulls]\n",
        "            except:\n",
        "              subjects = []\n",
        "            article_names_subjects.append(subjects)\n",
        "        return article_names_subjects\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1EROT7ixxie"
      },
      "source": [
        "**Import and Clean Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYiftX37x3f3"
      },
      "source": [
        "%%capture\n",
        "# import \n",
        "cols = ['year', 'month', 'day', 'title', 'publication', 'article']\n",
        "news = pandas.read_csv('/Data/Articles/all_the_news50k.csv', usecols=cols, engine='python', encoding='utf-8', error_bad_lines=False)\n",
        "\n",
        "# some basic cleaning\n",
        "news = news[news['article'].str.count(' ') >= 50]\n",
        "news = news.dropna(subset=['year', 'month', 'day', 'title', 'publication', 'article'])\n",
        "news = news[['article_id','year', 'month', 'day', 'title', 'publication', 'article']]\n",
        "\n",
        "# remove whitespace and punctatation within quotations\n",
        "news['article'] = news['article'].replace('\\s+', ' ', regex=True)\n",
        "news['article'] = news['article'].replace(r'[“|”|]', '\"', regex=True)\n",
        "news['article'] = news['article'].replace(r'(?!(([^\"]*\"){2})*[^\"]*$)[\\?|\\.|\\!]', '', regex=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRTpzInZc7ML"
      },
      "source": [
        "# load processors\n",
        "nlp_stanza = stanza.Pipeline('en', processors='tokenize, ner', tokenize_no_ssplit = True)\n",
        "nlp_spacy = spacy.load('en_core_web_sm')\n",
        "\n",
        "# getting subjects chunk by chunk (better for memory purposes)\n",
        "count = 0\n",
        "for news_chunk in np.array_split(news, 500):\n",
        "    try:\n",
        "        # get names from each article\n",
        "        news_chunk['article_names_partial'] = Article.names(news_chunk['article'])\n",
        "        news_chunk['article_names_full']=news_chunk['article_names_partial'].apply(lambda x: partial_to_full(x))\n",
        "        news_chunk = news_chunk[news_chunk['article_names_partial'].map(lambda d: len(d)) >= 1]\n",
        "        \n",
        "        # exploding datset to sentences and extracting nominal subjects\n",
        "        news_chunk = split_sentences(data = news_chunk, textcol = 'article')\n",
        "        news_chunk = news_chunk[['year', 'month', 'day', 'publication', 'article_id', 'sentence_id', 'sentence','article_names_partial', 'article_names_full']]\n",
        "        news_chunk = news_chunk[news_chunk.apply(lambda x: any(name in x.sentence for name in x.article_names_partial), axis=1)]\n",
        "        news_chunk['sentence_subject_names'] = Article.subject_names(news_chunk['sentence'], searchnames=news_chunk['article_names_partial'])\n",
        "        news_chunk = news_chunk[news_chunk['sentence_subject_names'].map(lambda d: len(d)) >= 1]\n",
        "        news_chunk['sentence_subject_names']=news_chunk[['sentence_subject_names', 'article_names_full']].apply(lambda x: partial_to_full(x[0], fullnames=x[1]), axis = 1)\n",
        "\n",
        "    \n",
        "        # download chunk\n",
        "        news_chunk.to_csv('Data/Sentences/sentences_' + str(count) + '.csv', index = False)\n",
        "        count = count + 1\n",
        "    except:\n",
        "        print('issue with chunk #' + str(count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenate sentence chunks together\n",
        "path = 'Data/Sentences'\n",
        "all_files = glob.glob(path + \"/*.csv\")\n",
        "\n",
        "li = []\n",
        "for filename in all_files:\n",
        "    df = pandas.read_csv(filename, index_col=None, header=0)\n",
        "    li.append(df)\n",
        "    \n",
        "# write dataframe\n",
        "sentences = pandas.concat(li, axis=0, ignore_index=True)\n",
        "sentences.to_csv('Data/sentences.csv', index = False)"
      ],
      "metadata": {
        "id": "hzkbpd-ueiAq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}